

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Deep learning via polynomials &mdash; Effective Quadratures v8.0 documentation</title>
  

  
  
    <link rel="shortcut icon" href="../_static/logo.png"/>
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/language_data.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Contact" href="contact.html" />
    <link rel="prev" title="Vector-valued dimension reduction" href="tutorial_14.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: grey" >
          

          
            <a href="index.html" class="icon icon-home"> Effective Quadratures
          

          
            
            <img src="../_static/logo.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                8.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Effective Quadratures</a></li>
<li class="toctree-l1"><a class="reference internal" href="modules.html">Modules</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="tutorials.html">Quick start guide</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="tutorial_1.html">Defining a parameter</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_2.html">Generating univariate quadrature rules</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_3.html">Constructing orthogonal polynomials</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_4.html">Computing moments</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_4b.html">Multi-index sets</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_5.html">Sparse and tensor grid quadrature rules</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_6a.html">Polynomial regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_6.html">Polynomial regression for time varying data</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_7.html">Polynomial least squares approximations</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_8.html">Polynomials via compressive sensing</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_9.html">Computing Sobol’ (sensitivity) indices</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_9b.html">Higher order Sobol’ indices</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_10.html">Nataf transform for correlated inputs</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_11.html">Active subspaces with polynomial approximations</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_12.html">Polynomial variable projection</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_14.html">Vector-valued dimension reduction</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Deep learning via polynomials</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="contact.html">Contact</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Effective Quadratures</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
          <li><a href="tutorials.html">Quick start guide</a> &raquo;</li>
        
      <li>Deep learning via polynomials</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/_documentation/tutorial_15.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="deep-learning-via-polynomials">
<h1>Deep learning via polynomials<a class="headerlink" href="#deep-learning-via-polynomials" title="Permalink to this headline">¶</a></h1>
<p>In this tutorial, we extend the generality of our model by studying neural network architectures based on polynomials. A neural network is formed by one or more <em>hidden layers</em>, which are composed of multiple nodes called <em>perceptrons</em>. A perceptron takes in a linear combination (<span class="math notranslate nohighlight">\(\mathbf{w}\)</span>) of the input (<span class="math notranslate nohighlight">\(\mathbf{x}\)</span>) and passes it through a non-linear transformation (<span class="math notranslate nohighlight">\(p\)</span>). The latter is called the <em>activation function</em> of the perceptron. The following figure illustrates the structure of a perceptron.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/perceptron.png"><img alt="../_images/perceptron.png" src="../_images/perceptron.png" style="width: 652.0px; height: 275.0px;" /></a>
</div>
<p>Common activation functions include the sigmoid function, hyperbolic tangent function, and the rectified linear unit (ReLU). In this implementation, we use orthogonal polynomials as the activation function. In practice, perceptrons are usually connected together in multiple hidden layers, similar to the illustration below:</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/nn.png"><img alt="../_images/nn.png" src="../_images/nn.png" style="width: 551.0px; height: 256.0px;" /></a>
</div>
<p>The number of perceptrons in each layer, and the connectivity between the perceptrons can vary in practice. In the case of a single hidden layer, the model takes the form of a “multi-ridge function” with polynomials,</p>
<div class="math notranslate nohighlight">
\[y = p_1(\mathbf{w}_1^T \mathbf{x}) + p_2(\mathbf{w}_2^T \mathbf{x}) + ... + p_n(\mathbf{w}_n^T \mathbf{x}).\]</div>
<p>In this model, the parameters are given by the ridge directions <span class="math notranslate nohighlight">\(\mathbf{w}_i\)</span> (or “weights”) in each perceptron, together with the polynomial coefficients of each <span class="math notranslate nohighlight">\(p_i\)</span>. Given some data <span class="math notranslate nohighlight">\((\mathbf{x}^{(i)}, t^{(i)})_{i=1}^N\)</span>, how do we fit the parameters? We can minimize the mean squared error inferred from our training data,</p>
<div class="math notranslate nohighlight">
\[L = \sum_{i=1}^N (t^{(i)} - y(\mathbf{x}^{(i)}))^2\]</div>
<p>A wealth of optimization techniques can be found in the neural networks literature. A technique known as <em>error backpropagation</em> allows efficient calculation of gradients of the loss with respect to network parameters, which allows the use of many first-order methods. The simplest first-order method is the <em>steepest descent</em> method, which updates the weights at iteration math:<cite>tau</cite> according to the following</p>
<div class="math notranslate nohighlight">
\[\mathbf{w}_i [\tau + 1] = \mathbf{w}_i[\tau] - \eta \left.\frac{\partial L}{\partial \mathbf{w}_i}\right|_{\tau}\]</div>
<p>The polynomial coefficients are updated similarly. Here, <span class="math notranslate nohighlight">\(\eta\)</span> is known as the <em>learning rate</em>, which dictates the step size at each iteration. It is a <em>hyperparameter</em> which needs to be decided manually and tuned depending on application. Other methods of optimization include:</p>
<ol class="arabic simple">
<li><p>Momentum-based steepest descent, where we add a fraction of the previous change in parameters to the current step:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[\mathbf{w}_i [\tau + 1] = \mathbf{w}_i[\tau] - \eta \left.\frac{\partial L}{\partial \mathbf{w}_i}\right|_{\mathbf{w}_i, \alpha_{ij}} - \beta(\mathbf{w}_i [\tau + 1] - \mathbf{w}_i[\tau])\]</div>
<p>This introduces another hyperparameter <span class="math notranslate nohighlight">\(\beta\)</span>.</p>
<ol class="arabic simple" start="2">
<li><p>Adaptive learning rate: When the previous change causes the loss to decrease, increase the learning rate by 10%; otherwise, decrease it by 50%. The fractional increase/decrease of the learning rate are also hyperparameters.</p></li>
</ol>
<p>There are also variants of these techniques, such as stochastic gradient descent, which selects random samples to evaluate the gradient at each step. Second order methods (such as Newton-Raphson) can also be used, but backpropagation becomes more complicated, and the inversion of the Hessian can be costly.</p>
<p>In EQ, polynomial neural networks are implemented in the Polynet class.</p>
<p><strong>Code Implementation</strong></p>
<p>We demonstrate the polynet routines through a dataset obtained from [1], concerning the system efficiency of a fan blade parameterized by 25 variables. We are given 548 points in total. We load this data into the arrays X and Y:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">equadratures</span> <span class="k">import</span> <span class="o">*</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">loadtxt</span><span class="p">(</span><span class="s1">&#39;h_X.dat&#39;</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">loadtxt</span><span class="p">(</span><span class="s1">&#39;h_Y.dat&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Then, we partition the data into a training set and verification set. We will fit the data on the training set, and evaluate the goodness of fit using the verification set.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">n_data</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">n_data</span><span class="p">,</span> <span class="mi">400</span><span class="p">,</span><span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">train</span><span class="p">]</span>
<span class="n">Y_train</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[</span><span class="n">train</span><span class="p">]</span>
<span class="n">ver</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_data</span><span class="p">)</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">train</span><span class="p">])</span>
<span class="n">X_ver</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">ver</span><span class="p">]</span>
<span class="n">Y_ver</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[</span><span class="n">ver</span><span class="p">]</span>
</pre></div>
</div>
<p>Now, we construct an instance of the Polynet class and call the fit method to optimize the parameters.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">net</span> <span class="o">=</span> <span class="n">Polynet</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">Y_train</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">max_iters</span><span class="o">=</span><span class="mi">30000</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span>  <span class="n">opt</span><span class="o">=</span><span class="s1">&#39;adapt&#39;</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">net</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
<p>In this example, we will use a single layer network; “2” refers to the number of hidden units in this layer. If we were to use, say, two layers with 3 perceptrons in each layer, we would put “[3,3]” in this argument. “max_iters” specify the number of maximum iterations to run the optimizer. Since we choose to use adaptive learning rate here, the “learning_rate” parameter specifies the <em>initial</em> learning rate, which will evolve across iterations. “verbose” lets us know the progress of the optimizer by displaying the current loss and learning rate.</p>
<p>After the optimizer is done, we can examine the goodness of fit.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">Y_train</span><span class="p">,</span><span class="n">net</span><span class="o">.</span><span class="n">evaluate_fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">),</span> <span class="n">s</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">Y_ver</span><span class="p">,</span> <span class="n">net</span><span class="o">.</span><span class="n">evaluate_fit</span><span class="p">(</span><span class="n">X_ver</span><span class="p">),</span><span class="n">s</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;data&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;prediction&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/result.png"><img alt="../_images/result.png" src="../_images/result.png" style="width: 640.0px; height: 480.0px;" /></a>
</div>
<p>The blue dots show the training data and the orange dots the verification data. Though the fit is not perfect (the cluster of points near the top of the range may have caused some difficulties for the optimizer). The verification data is fit with similar accuracy to the training data, showing that the model has not overfit.</p>
<p><strong>References</strong></p>
<dl class="footnote brackets">
<dt class="label" id="id1"><span class="brackets">1</span></dt>
<dd><p>Seshadri, P., Shahpar, S., Constantine, P., Parks, G., Adams, M. Turbomachinery active subspace performance maps. Journal of Turbomachinery, 140(4), 041003. <a class="reference external" href="http://turbomachinery.asmedigitalcollection.asme.org/article.aspx?articleid=2668256">Paper</a></p>
</dd>
</dl>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="contact.html" class="btn btn-neutral float-right" title="Contact" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="tutorial_14.html" class="btn btn-neutral float-left" title="Vector-valued dimension reduction" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2016-2019 by Effective Quadratures

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>