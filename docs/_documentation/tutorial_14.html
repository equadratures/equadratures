

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Vector-valued dimension reduction &mdash; Effective Quadratures v8.0 documentation</title>
  

  
  
    <link rel="shortcut icon" href="../_static/logo.png"/>
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/language_data.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Polynomial neural networks" href="tutorial_15.html" />
    <link rel="prev" title="Polynomial variable projection" href="tutorial_12.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: grey" >
          

          
            <a href="index.html" class="icon icon-home"> Effective Quadratures
          

          
            
            <img src="../_static/logo.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                8.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Effective Quadratures</a></li>
<li class="toctree-l1"><a class="reference internal" href="modules.html">Modules</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="tutorials.html">Quick start guide</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="tutorial_1.html">Defining a parameter</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_2.html">Generating univariate quadrature rules</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_3.html">Constructing orthogonal polynomials</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_4.html">Computing moments</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_4b.html">Multi-index sets</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_5.html">Sparse and tensor grid quadrature rules</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_6a.html">Polynomial regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_6.html">Polynomial regression for time varying data</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_7.html">Polynomial least squares approximations</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_8.html">Polynomials via compressive sensing</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_9.html">Computing Sobol’ (sensitivity) indices</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_9b.html">Higher order Sobol’ indices</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_10.html">Nataf transform for correlated inputs</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_11.html">Active subspaces with polynomial approximations</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_12.html">Polynomial variable projection</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Vector-valued dimension reduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_15.html">Polynomial neural networks</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="contact.html">Contact</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Effective Quadratures</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
          <li><a href="tutorials.html">Quick start guide</a> &raquo;</li>
        
      <li>Vector-valued dimension reduction</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/_documentation/tutorial_14.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="vector-valued-dimension-reduction">
<h1>Vector-valued dimension reduction<a class="headerlink" href="#vector-valued-dimension-reduction" title="Permalink to this headline">¶</a></h1>
<p>In some applications, we may be interested in the approximation of vector-valued objectives. This allows us to study the dimension reducing subspaces of multiple scalar objectives, such as the lift and drag of an airfoil simultaneously. Alternatively, we may be interested in the discretized pressure profile over this airfoil. With an approach that extends the technique of <a class="reference external" href="https://effective-quadratures.github.io/Effective-Quadratures/_documentation/tutorial_11.html">active subspaces</a>, and the closely related ridge approximation, we can find subspaces within which the greatest variation of the vector-valued function will be contained. In other words, for a function <span class="math notranslate nohighlight">\(\mathbf{f}(\mathbf{x}): \mathbb{R}^d \rightarrow \mathbb{R}^n\)</span>, we want to find a ridge approximation</p>
<div class="math notranslate nohighlight">
\[\mathbf{f}(\mathbf{x}) \approx \mathbf{g}( \mathbf{U}^T \mathbf{x}),\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{U} \in \mathbb{R}^{k \times d}\)</span> with <span class="math notranslate nohighlight">\(k \ll d\)</span>. In Effective Quadratures, we base our approach on Zahm et al. [1] and study the vector gradient covariance matrix. Let us define the Jacobian matrix as</p>
<div class="math notranslate nohighlight">
\[\mathbf{J}(\mathbf{x}) = \left[\frac{\partial f_1}{\partial \mathbf{x}} , \frac{\partial f_2}{\partial \mathbf{x}} ,..., \frac{\partial f_n}{\partial \mathbf{x}} \right],\]</div>
<p>where <span class="math notranslate nohighlight">\(f_i\)</span> is the <span class="math notranslate nohighlight">\(i\)</span>-th output component of <span class="math notranslate nohighlight">\(\mathbf{f}\)</span>. This matrix is constructed by stacking together the gradient vectors <span class="math notranslate nohighlight">\(\frac{\partial f_i}{\partial \mathbf{x}}\)</span>.</p>
<p>So, how are the gradients evaluated? Assuming that automatic differentiation utilities or adjoints are not available, we can adopt an approach similar to that detailed in the <a class="reference external" href="https://effective-quadratures.github.io/Effective-Quadratures/_documentation/tutorial_11.html">active subspaces tutorial</a> , and assume that each <span class="math notranslate nohighlight">\(f_i\)</span> has already been approximated as a polynomial series,</p>
<div class="math notranslate nohighlight">
\[f_i(\mathbf{x}) \approx \sum_{j=1}^P a_j \phi_j(\mathbf{x}),\]</div>
<p>from which gradients are easily evaluated. Then, we can form the gradient covariance matrix <span class="math notranslate nohighlight">\(\mathbf{H}\)</span> as</p>
<div class="math notranslate nohighlight">
\[\mathbf{H} = \int \mathbf{J}(\mathbf{x}) \mathbf{R} \mathbf{J}(\mathbf{x})^T \rho(\mathbf{x}) d\mathbf{x}.\]</div>
<p>In this expression, the positive semi-definite matrix <span class="math notranslate nohighlight">\(\mathbf{R}\)</span> represents the weighting of objectives. We can place more weight on some objectives than others, analogous to a weighted objective function derived from multiple objectives in an optimization problem.</p>
<p>We can then compute the eigendecomposition of <span class="math notranslate nohighlight">\(\mathbf{H}\)</span> as in the scalar output case, and retain the eigenvectors corresponding to the largest eigenvalues as the columns of the dimension reducing matrix <span class="math notranslate nohighlight">\(\mathbf{U}\)</span>. A Monte-Carlo based strategy for vector-valued dimension reduction using EQ is then</p>
<ol class="arabic simple">
<li><p>For each component <span class="math notranslate nohighlight">\(f_i(\mathbf{x})\)</span> of the function <span class="math notranslate nohighlight">\(\mathbf{f}\)</span>, we fit a polynomial surrogate, <span class="math notranslate nohighlight">\(f_i(\mathbf{x}) \approx p_i(\mathbf{x}) = \sum_{j=1}^P a_j \phi_j(\mathbf{x})\)</span></p></li>
<li><p>Sample each polynomial at <span class="math notranslate nohighlight">\(M\)</span> points drawn from the input distribution <span class="math notranslate nohighlight">\(\rho(\mathbf{x})\)</span> and evaluate the gradient at each point.</p></li>
<li><p>Form the Jacobian matrix using the polynomial surrogates at each input point, <span class="math notranslate nohighlight">\(\mathbf{J}(\mathbf{x}_i) = \left[\frac{\partial p_1}{\partial \mathbf{x}}(\mathbf{x}_i) , \frac{\partial p_2}{\partial \mathbf{x}} (\mathbf{x}_i),..., \frac{\partial p_n}{\partial \mathbf{x}}  (\mathbf{x}_i)\right]\)</span> for <span class="math notranslate nohighlight">\(i = 1,...,M\)</span>.</p></li>
<li><p>Evaluate the vector gradient covariance matrix with a prescribed <span class="math notranslate nohighlight">\(\mathbf{R}\)</span>,</p></li>
</ol>
<div class="math notranslate nohighlight">
\[\mathbf{H} = \frac{1}{M}\sum_{i=1}^M \mathbf{J}(\mathbf{x}_i) \mathbf{R} \mathbf{J}(\mathbf{x}_i)^T\]</div>
<ol class="arabic simple" start="5">
<li><p>Compute the eigendecomposition of <span class="math notranslate nohighlight">\(\mathbf{H}\)</span> and partition the eigenvectors with large eigenvalues to obtain <span class="math notranslate nohighlight">\(\mathbf{U}\)</span>:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[\begin{split}\mathbf{H} = [\mathbf{U}\quad \mathbf{V}] \begin{bmatrix} \mathbf{\Lambda}_1 &amp; \\ &amp; \mathbf{\Lambda}_2 \end{bmatrix} [\mathbf{U} \quad \mathbf{V}]^T,\end{split}\]</div>
<p><strong>Code Implementation</strong></p>
<p>We consider a simple analytical example to demonstrate the use of our vector-valued dimension reduction routines. Consider the function <span class="math notranslate nohighlight">\(\mathbf{f}: [-1,1]^5 \rightarrow \mathbb{R}^2\)</span>, with <span class="math notranslate nohighlight">\(\mathbf{f}(\mathbf{x}) = [f_0(\mathbf{x}), f_1(\mathbf{x})]^T\)</span> where:</p>
<div class="math notranslate nohighlight">
\[f_0(\mathbf{x}) = \sin(\pi \mathbf{w}_0^T \mathbf{x})\]</div>
<div class="math notranslate nohighlight">
\[f_1(\mathbf{x}) = \exp(\mathbf{w}_1^T \mathbf{x})\]</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">equadratures</span> <span class="k">import</span> <span class="o">*</span>
<span class="n">d</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">2</span>

<span class="k">def</span> <span class="nf">f_0</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">w</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">f_1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">w</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">w</span><span class="p">))</span>
</pre></div>
</div>
<p>We define <span class="math notranslate nohighlight">\(\mathbf{W} = [\mathbf{w}_0, \mathbf{w}_1]\)</span> as random orthogonal vectors…</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">rand_norm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">d</span><span class="p">,</span><span class="n">n</span><span class="p">)</span>
<span class="n">W</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">qr</span><span class="p">(</span><span class="n">rand_norm</span><span class="p">)</span>
</pre></div>
</div>
<p>and use 1000 samples to construct seventh-degree polynomials for the component functions:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">poly_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">p</span> <span class="o">=</span> <span class="mi">7</span>
<span class="n">myBasis</span> <span class="o">=</span> <span class="n">Basis</span><span class="p">(</span><span class="s1">&#39;Total order&#39;</span><span class="p">,</span> <span class="p">[</span><span class="n">p</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">d</span><span class="p">)])</span>
<span class="n">params</span> <span class="o">=</span> <span class="p">[</span><span class="n">Parameter</span><span class="p">(</span><span class="n">order</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">distribution</span><span class="o">=</span><span class="s1">&#39;uniform&#39;</span><span class="p">,</span> <span class="n">lower</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">d</span><span class="p">)]</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">))</span>
<span class="n">Y_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
<span class="n">Y_train</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">apply_along_axis</span><span class="p">(</span><span class="n">f_0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">W</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])</span>
<span class="n">Y_train</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">apply_along_axis</span><span class="p">(</span><span class="n">f_1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">W</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
<span class="n">poly_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Polyreg</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">myBasis</span><span class="p">,</span> <span class="n">training_inputs</span><span class="o">=</span><span class="n">X_train</span><span class="p">,</span> <span class="n">training_outputs</span><span class="o">=</span><span class="n">Y_train</span><span class="p">[:,</span><span class="n">i</span><span class="p">],</span>
                        <span class="n">no_of_quad_points</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
</pre></div>
</div>
<p>Now we can call the <code class="code docutils literal notranslate"><span class="pre">vector_AS</span></code> method in the <code class="code docutils literal notranslate"><span class="pre">dr</span></code> class to compute the Jacobian and find the dimension reducing subspace <span class="math notranslate nohighlight">\(\mathbf{U}\)</span>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">my_dr</span> <span class="o">=</span> <span class="n">dr</span><span class="p">(</span><span class="n">training_input</span><span class="o">=</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">R</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="p">[</span><span class="n">eigs</span><span class="p">,</span> <span class="n">U</span><span class="p">]</span> <span class="o">=</span> <span class="n">my_dr</span><span class="o">.</span><span class="n">vector_AS</span><span class="p">(</span><span class="n">poly_list</span><span class="p">,</span> <span class="n">R</span><span class="o">=</span><span class="n">R</span><span class="p">)</span>
<span class="n">U</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">real</span><span class="p">(</span><span class="n">U</span><span class="p">)</span>
</pre></div>
</div>
<p>To verify whether our answer is reasonable, we can check the subspace distance between <span class="math notranslate nohighlight">\(\mathbf{U}\)</span> and the space spanned by the true dimension reducing subspace, <span class="math notranslate nohighlight">\(\text{span}(\mathbf{w}_0, \mathbf{w}_1)\)</span>:</p>
<div class="math notranslate nohighlight">
\[\text{dist}(\mathbf{U}, \mathbf{W}) = ||\mathbf{U}\mathbf{U}^T - \mathbf{W}\mathbf{W}^T||_2\]</div>
<p>where the 2-norm picks out the largest singular value of the argument.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">subspace_dist</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">A</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">B</span><span class="o">.</span><span class="n">T</span><span class="p">),</span> <span class="nb">ord</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">subspace_dist</span><span class="p">(</span><span class="n">W</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">U</span><span class="p">[:,:</span><span class="n">n</span><span class="p">])))</span>
</pre></div>
</div>
<p>We can also verify that the variation of both functions outside of <span class="math notranslate nohighlight">\(\mathbf{U}\)</span> is small. Here, for each of 10 sets of active coordinates, we sample 50 points in the inactive subspace (<span class="math notranslate nohighlight">\(\mathbf{V}\)</span>) using the “hit and run” algorithm from Python Active-subspaces Utility Library [2].</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">active_subspaces.domains</span> <span class="k">import</span> <span class="n">hit_and_run_z</span>
<span class="k">def</span> <span class="nf">harz</span><span class="p">(</span><span class="n">W1</span><span class="p">,</span><span class="n">W2</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">N</span><span class="p">):</span>
        <span class="n">U</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">W1</span><span class="p">,</span><span class="n">W2</span><span class="p">])</span>
        <span class="n">Z</span> <span class="o">=</span> <span class="n">hit_and_run_z</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">W1</span><span class="p">,</span> <span class="n">W2</span><span class="p">)</span>
        <span class="n">yz</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">y</span><span class="p">[:,</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">],</span> <span class="n">N</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">Z</span><span class="o">.</span><span class="n">T</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">yz</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>

<span class="n">N_inactive</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">plot_coords</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">N_inactive</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="n">rand_active_coords</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>

        <span class="n">new_X</span> <span class="o">=</span> <span class="n">harz</span><span class="p">(</span><span class="n">U</span><span class="p">[:,:</span><span class="n">n</span><span class="p">],</span> <span class="n">U</span><span class="p">[:,</span><span class="n">n</span><span class="p">:],</span> <span class="n">rand_active_coords</span><span class="p">,</span> <span class="n">N_inactive</span><span class="p">)</span>

        <span class="n">new_f0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">apply_along_axis</span><span class="p">(</span><span class="n">f_0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">new_X</span><span class="p">,</span> <span class="n">W</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">new_f1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">apply_along_axis</span><span class="p">(</span><span class="n">f_1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">new_X</span><span class="p">,</span> <span class="n">W</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span>

        <span class="n">plot_coords</span><span class="p">[:,</span><span class="mi">0</span><span class="p">,</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_f0</span>
        <span class="n">plot_coords</span><span class="p">[:,</span><span class="mi">1</span><span class="p">,</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_f1</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">plot_coords</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">t</span><span class="p">],</span> <span class="n">plot_coords</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">t</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/inactive.png"><img alt="../_images/inactive.png" src="../_images/inactive.png" style="width: 640.0px; height: 480.0px;" /></a>
</div>
<p>It can be seen that at each active coordinate, the variation in the inactive subspace is small.</p>
<p><strong>References</strong></p>
<dl class="footnote brackets">
<dt class="label" id="id1"><span class="brackets">1</span></dt>
<dd><p>Zahm, O.,  Constantine, P., Prieur, C. and Marzouk, Y., (2018). Gradient-based dimension reduction of multi-variate vector-valued functions. <a class="reference external" href="http://arxiv.org/abs/1801.07922">Preprint</a></p>
</dd>
<dt class="label" id="id2"><span class="brackets">2</span></dt>
<dd><p>Constantine, P., Howard, R., Glaws, A., Grey, Z., Diaz, P., &amp; Fletcher, L. (2016). Python Active-subspaces Utility Library. Zenodo. <a class="reference external" href="http://doi.org/10.5281/zenodo.158941">Code</a></p>
</dd>
</dl>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="tutorial_15.html" class="btn btn-neutral float-right" title="Polynomial neural networks" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="tutorial_12.html" class="btn btn-neutral float-left" title="Polynomial variable projection" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2016-2019 by Effective Quadratures

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>