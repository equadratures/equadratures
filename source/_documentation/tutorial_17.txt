Surrogate-Based Optimisation with Polynomials 
==============================================
In this tutorial, we demonstrate how one may use orthogonal polynomials constructed in Effective Quadratures for surrogate-based optimisation. One particularly significant benefit of using orthogonal polynomials for optimisation is that the polynomial basis terms :math:`\psi_{i}` and their derivatives :math:`\psi_{i}^{(d)}` may be easily found using the standard four-term recurrence

.. math::

		\sqrt{\beta_{i+1}} \psi_{i+1}^{(d)} = (r-\alpha_i) \psi_i^{(d)} - \sqrt{\beta_i} \psi_{i-1}^{(d)} + d \psi_i^{(d-1)}

for :math:`d,i \geq 0` where :math:`\psi_i^{(d)} \equiv 0` for :math:`n < d, n < 0`. The recurrence coefficients :math:`\alpha_i, \beta_i`, whose values are determined by the user-specified distribution of the weight function, indicate the class of orthogonal polynomial :math:`\psi_{i}`. Using this recurrence relation, derivatives of all orders may be calculated by Effective Quadratures very efficiently, allowing the user to have easy access to gradient information for optimisation of orthogonal polynomials. Effective Quadratures has a built-in :code:`Optimisation` class that will calculate derivatives of orthogonal polynomials and perform optimisation using the `minimize <https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html>`__ method from `Scipy optimize <https://docs.scipy.org/doc/scipy/reference/optimize.html>`__. Also included in this class is a simple derivative-free trust-region method for bound-constrained optimisation of nonlinear functions. This trust-region implementation constructs interpolating quadratic models using a set of :math:`\frac{(n+1)(n+1)}{2}` points.

To demonstrate a simple example of how this class can be used for surrogate-based optimisation, we consider the following constrained optimisation problem:

.. math::

	   	\begin{eqnarray}
      		\min_{x,y} 	\quad    	& (1-x)^2 + 100(y-x^2)^2 	\\
      		\textrm{ subject to } 	& x^3 - y \leq 0 	\\
      								& x + y = 2 				\\
      								& -1 \leq x \leq 1 			\\
      								& -1 \leq y \leq 1.
		\end{eqnarray}

First, let's use :code:`Poly` to construct the objective function and the first constraint in terms of Legendre polynomials defined over a total order basis.

.. code::

		import numpy as np
		import scipy as sp
		import equadratures as eq
		from scipy.stats import linregress

		def ObjFun(x):
		    f = np.zeros((x.shape[0]))
		    for i in range(x.shape[0]):
		        f[i] = sp.optimize.rosen(x[i,:])
		    return f

		def ConFun1(x):
		    g1 = np.zeros((x.shape[0]))
		    for i in range(g1.shape[0]):
		        g1[i] = x[i,0]**3 - x[i,1]
		    return g1

		n = 2
		N = 20

		#Evaluate the objective and constraint functions over a random DOE
		X = np.random.uniform(-1.0, 1.0, (N, n))
		f = ObjFun(X)
		g1 = ConFun1(X)
		#Split data into training and testing data
		indices = list(range(N))
		num_training_instances = int(0.8 * N)
		np.random.shuffle(indices)
		train_indices = indices[:num_training_instances]
		test_indices = indices[num_training_instances:]
		X_train, f_train, g1_train = X[train_indices, :], f[train_indices], g1[train_indices]
		X_test, f_test, g1_test = X[test_indices, :], f[test_indices], g1[test_indices]
		#Construct f using Legendre polynomials with a total order basis
		fParameters = [eq.Parameter(distribution='uniform', lower=-1., upper=1., order=4) for i in range(n)]
		myBasis = eq.Basis('total-order')
		fpoly = eq.Poly(fParameters, myBasis, method='least-squares', sampling_args={'mesh': 'user-defined', 'sample-points':X_train, 'sample-outputs':f_train})
		fpoly.set_model()
		#Construct g1 using Legendre polynomials with a total order basis
		g1Parameters = [eq.Parameter(distribution='uniform', lower=-1., upper=1., order=3) for i in range(n)]
		myBasis = eq.Basis('total-order')
		g1poly = eq.Poly(g1Parameters, myBasis, method='least-squares', sampling_args={'mesh': 'user-defined', 'sample-points':X_train, 'sample-outputs':g1_train}) 
		g1poly.set_model()

The coefficient of determination (R-squared) value of the fit of both of these functions can be computed via:

.. code::
		
		_,_,r_f,_,_ = linregress(fpoly.get_polyfit(X_test).flatten(), f_test.flatten())
		_,_,r_g1,_,_ = linregress(g1poly.get_polyfit(X_test).flatten(), g1_test.flatten())

		print(r_f**2)
		print(r_g1**2)

Both of these functions very nearly give a fit of :math:`1.0`, indicating an almost exact fit.

.. figure:: Figures/Rosenbrock.png
	:scale: 60 %
	
	Figure. Contours of the objective function.

Now that the nonlinear functions have been constructed using orthogonal polynomials, we can use :code:`Optimisation` to solve the aforementioned optimisation problem.

.. code::

		#Initialise optimisation problem by specifying optimisation method
		Opt = eq.Optimisation(method='trust-constr')
		#Add objective function by specifying Poly object
		Opt.add_objective(poly=fpoly)
		#Add nonlinear inequality constraints lb <= g1poly <= ub
		Opt.add_nonlinear_ineq_con(poly={'poly': g1poly, 'bounds': [-np.inf, 0.0]})
		#Add linear equality constraints Ax = b
		Opt.add_linear_eq_con(np.array([1.0,1.0]), np.array([2.0]))
		#Add lower and upper bounds
		Opt.add_bounds(-np.ones(n), np.ones(n))
		#Initialise starting point
		x0 = np.zeros(n)
		#Solve optimisation problem
		sol = Opt.optimise(x0)

		print(sol['x'])
		print(sol['fun'])

The solution to the above surrogate-based optimisation problem is dependent on the points which are used to train the model. However, the solution will very closely responds to the true optimal solution of :math:`0` found at :math:`x = [1,1]`.

Alternatively, if one already has access to function and derivatives values for a quantity of interest, one may not need to construct a :code:`Poly` object for the function. In these cases, user-provided functions may be supplied to the optimisation routine. The following code demonstrates how to do this for the first constraint of the same optimisation problem.

.. code::

		def ConFun1_Deriv(x):
		    return np.array([3.0*x[0]**2, -1.0])

		def ConFun1_Hess(x):
		    g_Hess = np.zeros((2, 2))
		    g_Hess[0, 0] = 6.0*x[0]
		    return g_Hess

		#Construct lambda functions of the constraint, its derivative, and its Hessian
		g1Func = lambda x: -ConFun1(x.reshape(1,-1))
		g1Grad = lambda x: -ConFun1_Deriv(x.flatten())
		g1Hess = lambda x: -ConFun1_Hess(x.flatten())
		#Initialise optimisation problem by specifying optimization method
		Opt = eq.Optimisation(method='trust-constr')
		#Add objective function by specifying Poly object
		Opt.add_objective(poly=fpoly)
		#Add lower and upper bounds
		Opt.add_bounds(-np.ones(n), np.ones(n))
		#Add linear equality constraints Ax = b
		Opt.add_linear_eq_con(np.array([1.0, 1.0]), np.array([2.0]))
		#Add nonlinear inequality constraints lb <= g1Func <= ub
		Opt.add_nonlinear_ineq_con(custom={'function': g1Func, 'jac_function': g1Grad, 'hessFunction': g1Hess})
		#Initialize starting point
		x0 = np.zeros(n)
		#Solve optimisation problem
		sol = Opt.optimise(x0)

		print(sol['x'])
		print(sol['fun'])

Just as in the previous case, the returned solution is dependent on the sample points, but it will be very close to the true optimal solution of :math:`0` found at :math:`x = [1,1]`.

The main benefit of using Effective Quadratures for optimisation is best realized in cases where derivatives are not known a priori or are very expensive to calculate. Such situations are commonplace in the scientific community e.g. for 'black-box' functions whose values are obtained through the use of expensive computer simulations. Derivative-free optimisation strategies, such as stochastic optimisation or Bayesian optimisation, may be used in such situations; however, these methods may not scale very well for problems of moderate to high dimension. On the other hand, using techniques available within Effective Quadratures, one can readily construct surrogate models of the function of interest using orthogonal polynomials and then optimise over the surrogate to approximate the optimal solution, even for high-dimensional functions.

To demonstrate this, we consider the following constrained optimisation problem:

.. math::

	   	\begin{eqnarray}
      		\min_{\mathbf{x}} 	\quad    	& \sum_{i=1}^n 3x_i^2 + 2x_i	\\
      		\textrm{ subject to } 	& \mathbf{x}^T \mathbf{x} \leq 4.
		\end{eqnarray}

Although, the gradients of these functions can be easily calculated analytically, we will show that if `COBYLA <https://en.wikipedia.org/wiki/COBYLA>`__ (a very common derivative-free optimisation strategy) is used, the number of function evaluations can be prohibitively high. 

The new objective and constraint can be defined using the following: 

.. code::

		def ObjFun2(x):
		    f = np.zeros(x.shape[0])
		    for i in range(x.shape[0]):
		        f[i] = np.sum(3.0*np.power(x[i,:], 2) + 2.0*x[i,:])
		    return f

		def ConFun2(x):
		    g = np.zeros((x.shape[0]))
		    for i in range(g.shape[0]):
		        g[i] = np.sum(np.power(x[i,:],2))
		    return g 

We call SciPy implementation of COBYLA using the following code:

.. code::

		constraints = {'type': 'ineq', 'fun': lambda x: ConFun2(x.reshape(1,-1)) - 4.0}
		sol = sp.optimize.minimize(lambda x: ObjFun2(x.reshape(1,-1)), np.zeros(n), method='COBYLA', constraints=constraints)
		xopt = sol['x']
		print(ObjFun2(xopt.reshape(1,-1)))
		print(ConFun2(xopt.reshape(1,-1)))
		print(sol['nfev'])

On the other hand, both of these functions are sparse (i.e. most of the polynomial coefficients are zero), so we can use the :code:`compressive-sensing` method within Effective Quadratures to construct accurate surrogates and perform surrogate-based optimisation using the following code:

.. code::

		n = 20
		N = 150

		X = np.random.uniform(-1.0, 1.0, (N, n))
		f = ObjFun2(X)
		g2 = ConFun2(X)

		fParameters = [eq.Parameter(distribution='uniform', lower=-1., upper=1., order=2) for i in range(n)]
		myBasis = eq.Basis('total-order')
		fpoly = eq.Poly(fParameters, myBasis, method='compressive-sensing', sampling_args={'mesh': 'user-defined', 'sample-points':X, 'sample-outputs':f})
		fpoly.set_model()

		g2Parameters = [eq.Parameter(distribution='uniform', lower=-1., upper=1., order=2) for i in range(n)]
		myBasis = eq.Basis('total-order')
		g2poly = eq.Poly(g2Parameters, myBasis, method='compressive-sensing', sampling_args={'mesh': 'user-defined', 'sample-points':X, 'sample-outputs':g2}) 
		g2poly.set_model()

		#Initialise optimization problem by specifying optimisation method
		Opt = Optimisation(method='SLSQP')
		#Add objective function by specifying Poly object
		Opt.add_objective(Poly=fpoly)
		#Add nonlinear inequality constraints lb <= g2poly <= ub
		Opt.add_nonlinear_ineq_con(poly={'poly': g2poly, 'bounds': [-np.inf,4.0]})
		#Solve optimisation problem
		sol2 = Opt.optimise(np.zeros(n))
		xopt2 = sol2['x']
		fopt2 = sol2['fun']
		print(ObjFun2(xopt2.reshape(1,-1)))
		print(ConFun2(xopt2.reshape(1,-1)))
		print(sol2['nfev'])

.. list-table:: Optimisation results
   :widths: 25 25 25 25
   :header-rows: 1

   * - Optimisation strategy
     - Function evaluations
     - Solution
     - Constraint value
   * - COBYLA
     - 551
     - -5.889 
     - 4.0
   * - Surrogate-based optimisation w/ EQ
     - 150
     - -6.655
     - 2.134

The above table demonstrates the possible benefits of using Effective Quadratures for surrogate-based optimisation, as a better solution is obtained with far fewer function evaluations. It should be noted that the effectiveness of this approach is highly dependent on the accuracy of the surrogate models. In this rather contrived example, orthogonal polynomials defined over the domain of interest provide a very good approximation of the true function; however, in many other cases, this is not necessarily true. In these cases, we can use the :code:`trust-region` method within the :code:`Optimisation` class for bound-constrained optimisation problems. 